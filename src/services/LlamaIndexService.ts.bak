/**
 * LlamaIndex Vector Search Service
 * Provides embedding generation and vector similarity search using LlamaIndex + LanceDB + HuggingFace
 * Modern implementation using LlamaIndex for multi-modal capabilities
 */

import {
  Document,
  VectorStoreIndex,
  SimpleVectorStore,
  Settings,
  IndexStructType,
  type BaseNode,
  type NodeWithScore,
  type Metadata,
} from 'llamaindex';
import { HuggingFaceEmbedding, HuggingFaceEmbeddingModelType } from '@llamaindex/huggingface';
import { HfInference } from '@huggingface/inference';
import * as lancedb from '@lancedb/lancedb';
import { join } from 'path';
import { homedir } from 'os';
import { existsSync, mkdirSync } from 'fs';
import { Logger } from '../utils/logger.js';
import type { DatabaseManager } from '../database/index.js';
import type { Server } from '@modelcontextprotocol/sdk/server/index.js';

export interface LlamaIndexConfig {
  dataPath?: string;
  embeddingModel?: string;
  huggingFaceToken?: string;
  chunkSize?: number;
  chunkOverlap?: number;
  temperature?: number;
  mcpServer?: Server; // MCP server for sampling requests
}

export interface VectorDocument {
  id: string;
  content: string;
  metadata?: Record<string, any>;
  type?: 'text' | 'image' | 'audio' | 'video' | 'multimodal';
}

export interface VectorSearchResult {
  id: string;
  content: string;
  metadata?: Record<string, any>;
  score: number;
  nodeId?: string;
}

export interface ChatResponse {
  response: string;
  sourceNodes?: NodeWithScore[];
  metadata?: Record<string, any>;
}

export class LlamaIndexService {
  private vectorIndex: VectorStoreIndex | null = null;
  private embedding: HuggingFaceEmbedding | null = null;
  private lanceConnection: lancedb.Connection | null = null;
  private lanceTable: lancedb.Table | null = null;
  private logger: Logger;
  private config: LlamaIndexConfig;
  private dataPath: string;
  private hfInference: HfInference | null = null;
  private mcpServer: Server | null = null;

  constructor(
    private db: DatabaseManager,
    config: LlamaIndexConfig = {}
  ) {
    this.logger = new Logger('llamaindex-service');
    this.config = {
      embeddingModel: 'sentence-transformers/all-MiniLM-L6-v2',
      chunkSize: 512,
      chunkOverlap: 50,
      temperature: 0.1,
      ...config
    };

    // Set up data directory for LanceDB (unified path)
    this.dataPath = config.dataPath || join(homedir(), '.mcptools', 'lancedb');
    this.ensureDataDirectory();

    // Initialize HuggingFace if token provided
    if (this.config.huggingFaceToken) {
      this.hfInference = new HfInference(this.config.huggingFaceToken);
    }

    // Store MCP server reference for sampling
    this.mcpServer = this.config.mcpServer || null;

    this.logger.info('LlamaIndexService initialized', {
      dataPath: this.dataPath,
      embeddingModel: this.config.embeddingModel,
      hasHfToken: !!this.config.huggingFaceToken,
      hasMcpServer: !!this.mcpServer
    });
  }

  /**
   * Initialize the service with LlamaIndex + LanceDB integration
   */
  async initialize(): Promise<{ success: boolean; error?: string }> {
    try {
      if (this.vectorIndex && this.lanceConnection) {
        return { success: true };
      }

      // 1. Initialize HuggingFace embedding model
      this.embedding = new HuggingFaceEmbedding({
        modelType: HuggingFaceEmbeddingModelType.XENOVA_ALL_MINILM_L6_V2
      });

      // 2. Initialize LanceDB connection (skip LlamaIndex LLM setup)
      this.lanceConnection = await lancedb.connect(this.dataPath);
      
      // 4. Create or get LanceDB table for vectors
      try {
        this.lanceTable = await this.lanceConnection.openTable('llamaindex_vectors');
        this.logger.info('Opened existing LanceDB table');
      } catch (error) {
        // Create new table if it doesn't exist
        const sampleData = [{
          id: 'init',
          content: 'Initialization document',
          vector: new Array(384).fill(0), // HuggingFace model dimension
          metadata: JSON.stringify({ init: true })
        }];
        
        this.lanceTable = await this.lanceConnection.createTable('llamaindex_vectors', sampleData);
        this.logger.info('Created new LanceDB table');
      }

      // 5. Skip VectorStoreIndex - we'll use direct LanceDB + MCP Sampling
      // this.vectorIndex = await VectorStoreIndex.fromDocuments([]);

      this.logger.info('LlamaIndex + LanceDB integration initialized', {
        indexStructType: IndexStructType.LANCEDB,
        embeddingModel: this.config.embeddingModel,
        dataPath: this.dataPath
      });
      
      return { success: true };

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Unknown error';
      this.logger.error('Failed to initialize LlamaIndex + LanceDB service', { error: errorMsg });
      return { success: false, error: errorMsg };
    }
  }

  /**
   * Add documents to the vector index
   */
  async addDocuments(
    documents: VectorDocument[]
  ): Promise<{ success: boolean; addedCount: number; error?: string }> {
    try {
      const result = await this.initialize();
      if (!result.success) {
        return { success: false, addedCount: 0, error: result.error };
      }

      // Directly add documents to LanceDB (skip LlamaIndex processing)
      await this.syncDocumentsToLanceDB(documents);

      this.logger.info(`Added ${documents.length} documents to LanceDB`);

      // Update database with vector IDs if we have a documentation_entries table
      await this.updateDocumentationVectorIds(documents);

      return {
        success: true,
        addedCount: documents.length
      };

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Unknown error';
      this.logger.error('Failed to add documents', { error: errorMsg });
      return {
        success: false,
        addedCount: 0,
        error: errorMsg
      };
    }
  }

  /**
   * Search for similar documents using manual vector similarity (bypass LlamaIndex)
   */
  async searchSimilar(
    query: string,
    limit: number = 10,
    threshold?: number
  ): Promise<VectorSearchResult[]> {
    try {
      const result = await this.initialize();
      if (!result.success) {
        throw new Error(`Initialization failed: ${result.error}`);
      }

      // Use direct LanceDB search instead of LlamaIndex
      return await this.searchLanceDB(query, limit, threshold);

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Unknown error';
      this.logger.error('Search failed', { error: errorMsg });
      throw new Error(`Search operation failed: ${errorMsg}`);
    }
  }

  /**
   * Chat with documents using MCP Sampling + retrieved context
   */
  async chatWithDocuments(
    query: string,
    conversationHistory?: string[]
  ): Promise<ChatResponse> {
    try {
      const result = await this.initialize();
      if (!result.success) {
        throw new Error(`Initialization failed: ${result.error}`);
      }

      if (!this.mcpServer) {
        throw new Error('MCP Server not configured for chat functionality');
      }

      // 1. Search for relevant documents using vector similarity
      const relevantDocs = await this.searchLanceDB(query, 5, 0.3);
      
      // 2. Build context from retrieved documents
      const context = relevantDocs
        .map((doc, index) => `[Document ${index + 1}]: ${doc.content}`)
        .join('\n\n');

      // 3. Build conversation messages for MCP Sampling
      const messages = [];
      
      // Add conversation history if provided
      if (conversationHistory && conversationHistory.length > 0) {
        conversationHistory.forEach((msg, index) => {
          messages.push({
            role: index % 2 === 0 ? 'user' : 'assistant',
            content: { type: 'text', text: msg }
          });
        });
      }

      // Add current query with context
      messages.push({
        role: 'user',
        content: {
          type: 'text',
          text: `Context Documents:
${context}

Question: ${query}

Please answer the question based on the provided context documents. Cite specific documents when possible.`
        }
      });

      // 4. Request completion from MCP client using sampling
      const samplingResult = await this.mcpServer.request(
        { method: 'sampling/createMessage' },
        {
          messages,
          systemPrompt: 'You are a helpful assistant that answers questions based on provided documents. Be accurate and cite your sources.',
          maxTokens: 1000,
          temperature: this.config.temperature || 0.1,
          includeContext: 'thisServer'
        }
      );

      const responseText = samplingResult.content?.text || 'No response generated.';

      this.logger.info('Generated MCP chat response', { 
        queryLength: query.length, 
        responseLength: responseText.length,
        relevantDocsFound: relevantDocs.length
      });

      return {
        response: responseText,
        sourceNodes: relevantDocs.map(doc => ({
          node: {
            id_: doc.id,
            text: doc.content,
            metadata: doc.metadata
          } as BaseNode,
          score: doc.score
        })) as NodeWithScore[],
        metadata: {
          model: 'mcp-sampling',
          timestamp: new Date().toISOString(),
          relevantDocuments: relevantDocs.length
        }
      };

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Unknown error';
      this.logger.error('MCP Chat failed', { error: errorMsg });
      throw new Error(`Chat operation failed: ${errorMsg}`);
    }
  }

  /**
   * Process multi-modal content (text, images, etc.)
   * For now handles as text - LlamaIndex supports multi-modal with proper models
   */
  async addMultiModalDocument(
    id: string,
    content: string,
    type: 'text' | 'image' | 'audio' | 'video' | 'multimodal',
    metadata?: Record<string, any>
  ): Promise<{ success: boolean; error?: string }> {
    try {
      // For now, handle all content as text
      // LlamaIndex supports multi-modal but requires specific models/embeddings
      const document: VectorDocument = {
        id,
        content,
        type,
        metadata: {
          ...metadata,
          contentType: type,
          processedAt: new Date().toISOString()
        }
      };

      const result = await this.addDocuments([document]);
      return {
        success: result.success,
        error: result.error
      };

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Unknown error';
      this.logger.error('Failed to add multi-modal document', { error: errorMsg });
      return {
        success: false,
        error: errorMsg
      };
    }
  }

  /**
   * Get statistics about the vector store
   */
  async getStats(): Promise<{
    totalDocuments: number;
    embeddingModel: string;
    dataPath: string;
    isInitialized: boolean;
  }> {
    try {
      const isInitialized = this.vectorIndex !== null;
      
      return {
        totalDocuments: 0, // LlamaIndex doesn't expose this directly
        embeddingModel: this.config.embeddingModel!,
        dataPath: this.dataPath,
        isInitialized
      };

    } catch (error) {
      this.logger.error('Failed to get stats', error);
      return {
        totalDocuments: 0,
        embeddingModel: this.config.embeddingModel!,
        dataPath: this.dataPath,
        isInitialized: false
      };
    }
  }

  /**
   * Test the service functionality (LanceDB + HuggingFace embeddings)
   */
  async testConnection(): Promise<{ connected: boolean; error?: string }> {
    try {
      const result = await this.initialize();
      if (!result.success) {
        return {
          connected: false,
          error: result.error
        };
      }

      // Test by adding a simple document and searching
      const testDoc: VectorDocument = {
        id: 'test_' + Date.now(),
        content: 'This is a test document for connection verification.',
        metadata: { test: true }
      };

      const addResult = await this.addDocuments([testDoc]);
      if (!addResult.success) {
        return {
          connected: false,
          error: `Failed to add test document: ${addResult.error}`
        };
      }

      // Test LanceDB search directly
      const searchResults = await this.searchLanceDB('test document', 1);
      
      this.logger.info('LanceDB + HuggingFace connection test successful', {
        searchResultsFound: searchResults.length
      });
      return { connected: true };

    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Connection test failed';
      this.logger.error('Connection test failed', { error: errorMsg });
      return {
        connected: false,
        error: errorMsg
      };
    }
  }

  /**
   * Update documentation_entries table with vector IDs
   */
  private async updateDocumentationVectorIds(
    documents: VectorDocument[]
  ): Promise<void> {
    try {
      // Check if documentation_entries table exists and has vector_id column
      const tableInfo = this.db.database.prepare("PRAGMA table_info(documentation_entries)").all();
      const hasVectorIdColumn = tableInfo.some((col: any) => col.name === 'vector_id');

      if (!hasVectorIdColumn) {
        // Add vector_id column if it doesn't exist
        this.db.database.prepare(`
          ALTER TABLE documentation_entries 
          ADD COLUMN vector_id TEXT
        `).run();
        
        this.logger.info('Added vector_id column to documentation_entries table');
      }

      // Update existing documentation entries with vector IDs
      const updateStmt = this.db.database.prepare(`
        UPDATE documentation_entries 
        SET vector_id = ? 
        WHERE id = ?
      `);

      for (const doc of documents) {
        try {
          updateStmt.run(doc.id, doc.metadata?.documentation_entry_id);
        } catch (error) {
          // Ignore errors for documents that don't exist in documentation_entries
          this.logger.debug(`No documentation entry found for vector ID ${doc.id}`);
        }
      }

    } catch (error) {
      this.logger.warn('Failed to update documentation vector IDs', error);
      // Don't throw - this is not critical for vector operations
    }
  }

  /**
   * Ensure data directory exists
   */
  private ensureDataDirectory(): void {
    if (!existsSync(this.dataPath)) {
      mkdirSync(this.dataPath, { recursive: true });
      this.logger.info('Created LlamaIndex data directory', { path: this.dataPath });
    }
  }

  /**
   * Sync documents to LanceDB for persistent vector storage
   */
  private async syncDocumentsToLanceDB(documents: VectorDocument[]): Promise<void> {
    if (!this.lanceTable || !this.embedding) {
      this.logger.warn('LanceDB table or embedding not initialized, skipping sync');
      return;
    }

    try {
      const lanceData = [];
      
      for (const doc of documents) {
        // Generate embedding for the document
        const embedding = await this.embedding.getTextEmbedding(doc.content);
        
        lanceData.push({
          id: doc.id,
          content: doc.content,
          vector: embedding,
          metadata: JSON.stringify(doc.metadata || {})
        });
      }

      // Add to LanceDB table
      await this.lanceTable.add(lanceData);
      this.logger.info(`Synced ${documents.length} documents to LanceDB`);
      
    } catch (error) {
      this.logger.error('Failed to sync documents to LanceDB', error);
      // Don't throw - this is not critical for LlamaIndex operation
    }
  }

  /**
   * Search LanceDB directly for high-performance vector similarity
   */
  async searchLanceDB(
    query: string,
    limit: number = 10,
    threshold?: number
  ): Promise<VectorSearchResult[]> {
    if (!this.lanceTable || !this.embedding) {
      throw new Error('LanceDB not initialized');
    }

    try {
      // Generate embedding for query
      const queryEmbedding = await this.embedding.getTextEmbedding(query);
      
      // Search LanceDB directly
      const results = await this.lanceTable
        .search(queryEmbedding)
        .limit(limit)
        .toArray();

      // Convert to VectorSearchResult format
      const searchResults: VectorSearchResult[] = results
        .filter(result => !threshold || result._distance >= threshold)
        .map(result => ({
          id: result.id,
          content: result.content,
          metadata: JSON.parse(result.metadata || '{}'),
          score: 1 - result._distance, // Convert distance to similarity
          nodeId: result.id
        }));

      this.logger.info(`LanceDB search found ${searchResults.length} results`);
      return searchResults;
      
    } catch (error) {
      const errorMsg = error instanceof Error ? error.message : 'Unknown error';
      this.logger.error('LanceDB search failed', { error: errorMsg });
      throw new Error(`LanceDB search failed: ${errorMsg}`);
    }
  }

  /**
   * Clean up resources
   */
  async shutdown(): Promise<void> {
    this.vectorIndex = null;
    this.embedding = null;
    this.hfInference = null;
    this.lanceConnection = null;
    this.lanceTable = null;
    this.logger.info('LlamaIndexService shutdown complete');
  }
}